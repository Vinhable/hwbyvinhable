{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homeworks - Questions",
      "provenance": [],
      "collapsed_sections": [
        "RRtdwG3HzInG",
        "5MpPSmX4zGuG",
        "8sph3jiZISkH",
        "VTBc5vVdzNJe",
        "qWx5LnjzzXQm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpoDip8bIGe"
      },
      "source": [
        "# Homeworks\n",
        "1. Find the answer to the question raised in the lab1\n",
        "\n",
        "    Some helpful resources:\n",
        "- DeepWalk: https://arxiv.org/pdf/1403.6652.pdf\n",
        "- Word2vec: https://arxiv.org/pdf/1301.3781.pdf\n",
        "- Repository Github of Word2vec at [this link](https://github.com/RaRe-Technologies/gensim)\n",
        "2. Implement a simple word2vec algorithm for the DeepWalk (Attributes for each node should be created).\n",
        "3. Use some libraries to solve a real problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWpAGFkTy9LW"
      },
      "source": [
        "# Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKZjx1qdgl5u"
      },
      "source": [
        "## Implement Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Gr0JI5qUUG"
      },
      "source": [
        "### Download data and install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AaWu-oYRTPB"
      },
      "source": [
        "!pip install karateclub==1.2.0\n",
        "!pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe8SGQmHNH1p"
      },
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naI1TUJORTPC"
      },
      "source": [
        "!gdown --id \"1RmrHId0d-uY7kJCSgCtNYbwYfp4Oum3c&export=download\"\n",
        "!unrar x -Y \"/content/lab3.rar\" -d \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRtdwG3HzInG"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7MLBGJpBjv-"
      },
      "source": [
        "# Task 1\n",
        "import networkx as nx\n",
        "from joblib import Parallel, delayed\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Task 2\n",
        "import json\n",
        "import umap\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from karateclub.utils.walker import RandomWalker\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MpPSmX4zGuG"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wM_CQ-MCMCu"
      },
      "source": [
        "def partition_num(num, workers):\n",
        "    if num % workers == 0:\n",
        "        return [num//workers]*workers\n",
        "    else:\n",
        "        return [num//workers]*workers + [num % workers]\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "def get_attributes_of_node(node_paths):\n",
        "  node_paths_attributes = []\n",
        "  # Get attribute (word) for each node\n",
        "  df_attr = pd.read_csv(\"lab3_attributes.csv\").astype(str)\n",
        "  dict_attr = {}\n",
        "  for i in range(len(df_attr)):\n",
        "    dict_attr[df_attr.iloc[i, 0]] = df_attr.iloc[i, 1]\n",
        "  for path in node_paths:\n",
        "    for index, node in enumerate(path):\n",
        "      path[index] = dict_attr[node] \n",
        "    node_paths_attributes.append(path)\n",
        "  return node_paths_attributes\n",
        "\n",
        "def preprocessing(sentences):\n",
        "    training_data = []\n",
        "    for sentence in sentences:\n",
        "        x = [word for word in sentence]\n",
        "        training_data.append(x)\n",
        "    return training_data\n",
        "       \n",
        "   \n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "    data = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "       \n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "              \n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "                if i!=j and j>=0 and j<len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word)\n",
        "            w2v.y_train.append(context)\n",
        "    w2v.initialize(V,data)\n",
        "   \n",
        "    return w2v.X_train,w2v.y_train   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sph3jiZISkH"
      },
      "source": [
        "### TO DO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSexrQ5EHt7D"
      },
      "source": [
        "class word2vec():\n",
        "    pass\n",
        "    # TO DO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTBc5vVdzNJe"
      },
      "source": [
        "### DeepWalk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjIDrqVB-vtm"
      },
      "source": [
        "class RandomWalker:\n",
        "  def __init__(self, G, num_walks, walk_length):\n",
        "      \"\"\"\n",
        "      :param G: Graph\n",
        "      :param num_walks: a number of walks\n",
        "      :param walk_length: Length of a walk. Each walk is considered as a sentence\n",
        "      \"\"\"\n",
        "      self.G = G\n",
        "      self.num_walks = num_walks\n",
        "      self.walk_length = walk_length\n",
        "\n",
        "\n",
        "  def deepwalk_walk(self, start_node):\n",
        "      \"\"\"\n",
        "      :param start_node: Starting node of a walk\n",
        "      \"\"\"\n",
        "      walk = [start_node]\n",
        "      while len(walk) < self.walk_length:\n",
        "          cur = walk[-1]\n",
        "          # Check if having any neighbors at the current node\n",
        "          cur_nbrs = list(self.G.neighbors(cur))\n",
        "          if len(cur_nbrs) > 0:\n",
        "              # Random walk with the probability of 1/d(v^t). d(v^t) is the node degree\n",
        "              walk.append(random.choice(cur_nbrs))\n",
        "          else:\n",
        "              break\n",
        "      return walk\n",
        "\n",
        "\n",
        "  def simulate_walks(self, workers=1, verbose=0):\n",
        "      \"\"\"\n",
        "      :param workers: a number of workers running in parallel processing\n",
        "      :param verbose: progress bar\n",
        "      \"\"\"\n",
        "      G = self.G\n",
        "      nodes = list(G.nodes())\n",
        "      results = Parallel(n_jobs=workers, verbose=verbose)(\n",
        "          delayed(self._simulate_walks)(nodes) for num in\n",
        "          partition_num(self.num_walks, workers))\n",
        "      walks = list(itertools.chain(*results))\n",
        "      return walks\n",
        "\n",
        "\n",
        "  # INFORMATION EXTRACTOR\n",
        "  def _simulate_walks(self, nodes):\n",
        "      walks = []\n",
        "      # Iterate all walks per vertex\n",
        "      for _ in range(self.num_walks):\n",
        "          random.shuffle(nodes)\n",
        "          # Iterate all nodes in a walk\n",
        "          for v in nodes:\n",
        "            walks.append(self.deepwalk_walk(start_node=v))\n",
        "      return walks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYUUSiDD-VdL"
      },
      "source": [
        "class DeepWalk:\n",
        "    def __init__(self, graph, walk_length, num_walks, workers=1):\n",
        "\n",
        "        self.graph = graph\n",
        "        self.w2v_model = None\n",
        "        self._embeddings = {}\n",
        "\n",
        "        self.walker = RandomWalker(graph, num_walks=num_walks, walk_length=walk_length)\n",
        "        self.walks = self.walker.simulate_walks(workers=workers, verbose=1)\n",
        "        self.sentences = get_attributes_of_node(self.walks)\n",
        "\n",
        "\n",
        "    def train(self, window_size=5, epochs=100):\n",
        "        print(\"Learning embedding vectors...\")\n",
        "        training_data = preprocessing(self.sentences)\n",
        "        w2v = word2vec(window_size, epochs)\n",
        "        prepare_data_for_training(training_data, w2v)\n",
        "        w2v.train() \n",
        "        print(\"Learning embedding vectors done!\")\n",
        "        self.w2v_model = w2v\n",
        "\n",
        "\n",
        "    def test(self, word):\n",
        "        print(self.w2v_model.predict(word,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWx5LnjzzXQm"
      },
      "source": [
        "### Run graph embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtwVs_UrAFqH"
      },
      "source": [
        "G = nx.read_edgelist('lab3_edgelist.txt',create_using=nx.DiGraph(),nodetype=None,data=[('weight',int)])# Read graph\n",
        "model = DeepWalk(G, walk_length=3, num_walks=10, workers=1)#init model\n",
        "model.train(window_size=5)# train model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKyjIZ5T2e9m"
      },
      "source": [
        "print(model.sentences)\n",
        "model.test(\"to\")\n",
        "model.test(\"this\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snCfP6XKiBcW"
      },
      "source": [
        "## TO DO: Solve a real problem using some libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3OKBtsrERdI"
      },
      "source": [
        "Goal: When we have a large graph dataset like the Facebook dataset below, we want to classify which company (node) will likely belong to a type of page. If we categorize well, we could apply marketing strategies in a domain on a company that we are surveying.\n",
        "Therefore, our task is to learn a model which can classify a company using related features.\n",
        "\n",
        "\n",
        "1. Analyze and visualize the dataset Facebook downloaded in [this website](https://snap.stanford.edu/data/facebook-large-page-page-network.html).\n",
        "2. Use DeepWalk to embed the graph\n",
        "3. Train a classifier to do the node classification task using the embedding graph from step 2.\n",
        "\n",
        "You can do many things with the data. I recommend that you could try many tasks with this data, not only the classification task. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdTHjzzg1OGo"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy0GThOYF2HQ"
      },
      "source": [
        "edges_path = 'facebook_edges.csv'\n",
        "targets_path = 'facebook_target.csv'\n",
        "features_path = 'facebook_features.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIjmgPL11URP"
      },
      "source": [
        "### Visualize datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOSNdGIeGwo_"
      },
      "source": [
        "Create a graph. If you want to use smaller graph, please try to create one. It will be lighter when running the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrHJTDiC1bUn"
      },
      "source": [
        "### Embedding graph using DeepWalk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyw2GTlRD-kX"
      },
      "source": [
        "Embedding graph using DeepWalk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaIerx0GQlTX"
      },
      "source": [
        "### Train a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbVeCfFmG70H"
      },
      "source": [
        "Train a classifer from the embedding graph to the target. Here we use the Random Forest classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITdR7O6VFtip"
      },
      "source": [
        "# THANK YOU\n",
        "Please dive more into the codes and papers if you are interested.\n",
        "\n",
        "Thank you for joining all the labs."
      ]
    }
  ]
}